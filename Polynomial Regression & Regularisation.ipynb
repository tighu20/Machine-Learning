{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 1: Polynomial Regression\n",
    "\n",
    "### A) Use the Auto dataset, find the test $R^2$ score of a linear regression model that predicts the miles per gallon (mpg) from the horsepower.\n",
    "\n",
    "### B) Use polynomial regression to include both the horsepower feature and $(horsepower)^2$ in the regression model. Find the $R^2$ metric. \n",
    "\n",
    "Hint: You can use [numpy.concatenate](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.concatenate.html). For example to add to an array U a column vector $W^2$, we can use X=np.concatenate((U,W**2),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for  linear regression 0.6217658811398383\n",
      "R2 score for polynomial regression 0.7271031504642005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import  r2_score\n",
    "\n",
    "\n",
    "\n",
    "AutoData=read_csv('Auto_modify.csv') # read the data\n",
    "\n",
    "X_auto_hp=AutoData.horsepower.values.reshape(-1,1) # define features: horsepower \n",
    "Y_auto_mpg=AutoData.mpg.values.reshape(-1,1) # define label: miles per gallon\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_auto_hp,Y_auto_mpg,random_state=0)\n",
    "linreg=LinearRegression().fit(X_train,Y_train)\n",
    "Target_predicted= linreg.predict(X_test)\n",
    "print(\"R2 score for  linear regression\",r2_score(Y_test,Target_predicted))\n",
    "\n",
    "AutoData['horsepower2']=AutoData.horsepower*AutoData.horsepower\n",
    "X = AutoData[['horsepower','horsepower2']].values\n",
    "Y = AutoData.mpg\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state=0)\n",
    "linreg=LinearRegression().fit(X_train,Y_train)\n",
    "Target_predicted= linreg.predict(X_test)\n",
    "print(\"R2 score for polynomial regression\",r2_score(Y_test,Target_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C)Use KNN regression to predict the miles per gallon(mpg) with K=7, and find $R^2$ metric in the following cases \n",
    "\n",
    "- One feature: Horsepower only\n",
    "\n",
    "- Two features: horsepower and $(horsepower)^2$ \n",
    "\n",
    "Hint: \n",
    "\n",
    "    Create KNN regression object using neighbors.KNeighborsRegressor:\n",
    "\n",
    "    knnRegression = neighbors.KNeighborsRegressor(n_neighbors=7)\n",
    "\n",
    "    Use the .fit and .score methods as before\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score for KNN Regression with linear feature 0.6010051329011783\n",
      "R2 Score for KNN Regression with quadratic feature 0.6147596717312067\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "# add your solution here\n",
    "\n",
    "AutoData['horsepower2']=AutoData.horsepower*AutoData.horsepower\n",
    "\n",
    "\n",
    "X = AutoData[['horsepower']].values\n",
    "Y = AutoData.mpg\n",
    "\n",
    "knnRegression= neighbors.KNeighborsRegressor(n_neighbors=7)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state=100)\n",
    "knnRegression.fit(X_train,Y_train)\n",
    "Target_predicted=knnRegression.predict(X_test)\n",
    "print(\"R2 Score for KNN Regression with linear feature\",r2_score(Y_test,Target_predicted))\n",
    "\n",
    "X = AutoData[['horsepower','horsepower2']].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state=100)\n",
    "knnRegression.fit(X_train,Y_train)\n",
    "Target_predicted=knnRegression.predict(X_test)\n",
    "print(\"R2 Score for KNN Regression with quadratic feature\",r2_score(Y_test,Target_predicted))\n",
    "\n",
    "# here the performance increased marginally while adding the quadratic feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMMENT on your results on (E) and (F): which model performs better? How does performance change when adding the quadratic feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2: Regularization\n",
    "\n",
    "### A) Use the Boston dataset, and use Ridge regression model with tuning parameter set to 100 (alpha =100). Find the $R^2$ score and number of non zero coefficients.\n",
    "\n",
    "###  B) Use Lasso regression instead of Ridge regression, also set the tuning parameter to 100. Find the $R^2$ score and number of non zero coefficients.\n",
    "\n",
    "### C) Change the tuning parameter of the Lasso model to a very low value (alpha =0.001). What is the $R^2$ score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score for ridge model 0.6916568212105104\n",
      "The number of non zero coeficient in Ridge model are 13\n",
      "R2 score for lasso model 0.2247796373828993\n",
      "The number of non zero coeficient in lasso model are 2\n",
      "R2 score for low alpha lasso model 0.7241866900918309\n",
      "The number of non zero coeficient in low alpha lasso model are 13\n",
      "R2 score for lasso2 model 0.7245555583182401\n",
      "The number of non zero coeficient in Ridge model are 13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import Lasso\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_boston()\n",
    "X=dataset.data\n",
    "Y=dataset.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state=100)\n",
    "\n",
    "RidgeModel=Ridge(alpha=100).fit(X_train, Y_train)                 # Ridge model with alpha =100\n",
    "print(\"R2 score for ridge model\",RidgeModel.score(X_test,Y_test))\n",
    "nozr=np.sum(RidgeModel.coef_!=0)\n",
    "print(\"The number of non zero coeficient in Ridge model are\",nozr)\n",
    "\n",
    "\n",
    "LassoModel=Lasso(alpha=100).fit(X_train, Y_train)                 # Lasso model with alpha =100\n",
    "print(\"R2 score for lasso model\",LassoModel.score(X_test,Y_test))\n",
    "nozl=np.sum(LassoModel.coef_!=0)\n",
    "print(\"The number of non zero coeficient in lasso model are\",nozl)\n",
    "\n",
    "Lassolowalpha=Lasso(alpha=0.001).fit(X_train, Y_train)          # Lasso model with alpha =0.001\n",
    "print(\"R2 score for low alpha lasso model\",Lassolowalpha.score(X_test,Y_test))\n",
    "nozl1=np.sum(Lassolowalpha.coef_!=0)\n",
    "print(\"The number of non zero coeficient in low alpha lasso model are\",nozl1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### D) Comment on your result. In this problem, do all feature seem important in making predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the lasso model, high value of alpha(100) leads to model which only has 2 non zero coefficients while a really low\n",
    "# value of alpha(0.001) leads to a model which has 13 non zero coefficients. The better R2 score(0.72 vs 0.22) for low aplha model \n",
    "# suggests that atleast more than 2 features are required to make the predication. The value of non zero coefficents depends \n",
    "# on alpha and whether the R2 score peaked with alpha=0.001 cant be said with complete certainity.Also features vary in\n",
    "# scale and making a model without feature scaling will have shortcomings. Though the R2 score from the low aplha lasso\n",
    "# model makes it seem like that all features are important in making predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
